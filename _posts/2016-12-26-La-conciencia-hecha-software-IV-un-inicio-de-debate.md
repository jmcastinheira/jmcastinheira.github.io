---
author: Jmcastinheira
date: 2016-12-26
generator: pandoc
title: La conciencia hecha software IV (un inicio de debate)
categories:
<<<<<<< HEAD
  - Concepto

=======
  - Blog
- Concepto
>>>>>>> 186db3ed77b40c7493a2fedc023e873cd977b3e0

---



Los proyectos abuelos de la inteligencia artificial, como venimos
viendo, se inician lentamente en la época de la ilustración; pero poco a
poco, la comunidad científica se va dando cuenta de que sus expectativas
eran demasiado ambiciosas y que nunca serían satisfechas. Como hemos
visto hubo varios toques de atención a este proyecto y tras [la
intervención de
Gödel](http://www.entelequia.info/la-conciencia-hecha-software-o-el-sueno-de-la-razon-iii#content-top)
queda herido de muerte; en este punto quedó definitivamente claro que no
es posible crear un sistema axiomático con amplia capacidad expresiva
que sea coherente y completo al mismo tiempo; si el sistema es
consistente me encontraré con que hay ideas verdaderas expresables en mi
lenguaje que no se pueden demostrar.

Antes de seguir avanzando por los derroteros de la Inteligencia
Artificial, aún tenemos que hablar de un genio llamado [Allan
Turing](http://es.wikipedia.org/wiki/Turing), me gustaría meditar en
alto sobre alguna cuestión que considero de importancia sobre la
significación de lo que ya hemos dicho anteriormente. Solicito la
colaboración de quien quiera participar porque ya os digo que este
artículo encierra mi opinión y yo no puedo más que expresar una opinión
intuitiva pero mis conocimientos de la lógica no llegan muy lejos; así
que este artículo está abierto al debate y la colaboración.

Lo que me interesa investigar son las relaciones entre la semántica y la
sintáctica; en lo que puedan afectar al programa de Hilbert y a la
respuesta de Gödel.

Mi intuición es que en el fondo, el programa de
[Hilbert](http://es.wikipedia.org/wiki/Hilbert) pretende encerrar todo
el conocimiento acerca del mundo en una estructura sintáctica; en
concreto en una teoría axiomática; si esto fuera posible, una vez creado
este sistema, el conocimiento se podría transmitir a cualquier persona
(e incluso máquina) simplemente transmitiendo el contenido de los
axiomas y dando unas sencillas normas de cálculo lógico a partir de las
cuales podamos extraer *nuevos* conocimientos de los axiomas.

Veamos lo que esto significa. Cualquier investigación sobre el tema de
la teoría sería una investigación lógica; esto significa que si yo tengo
una Teoría axiomática de la biología, por ejemplo, cualquier cosa que yo
quiera encontrar, pongamos «una cura para la gripe», la encontraría en
alguna de las ideas que se derivan lógicamente de los axiomas,
sencillamente no tendría que ir por el mundo testeando a ratas griposas
de laboratorio con distintos antígenos, sería suficiente con sentarme en
la comodidad de mi cuarto con un papel y un lápiz, unos cuantos axiomas,
y un poco de paciencia... Veamos, iría formulando ideas como «la
penicilina cura la gripe», «el acido acetilsalicílico cura la gripe» «La
vitamina C cura la gripe»... así recursivamente; luego vería si dichas
ideas se derivan o no lógicamente de los axiomas; si encuentro alguna
que se derive de los axiomas sencillamente ¡Habré encontrado la vacuna
de la gripe! y lo mejor de todo es que tendría que funcionar aunque no
la haya probado nunca; si por un casual no funcionara eso será que está
mal la teoría, pero no el método. Bien, ¿esto es intuitivo o
contraintiuitivo? Bueno, pensad que si yo conozco el peso de un objeto y
las características físicas de una grúa, puedo saber, de antemano, si la
grúa podrá levantar el peso, a cuánta altura podrá levantarlo, en cuánto
tiempo, si oscilará etc... y las operaciones que realizo para saberlo
son puro cálculo. Esta idea llevada a su mayor radicalidad permitiría
axiomatizar la física entera; cosa que a día de hoy, es una ambición
perfectamente reconocida por algunos físicos teóricos, que de paso
anuncian la muerte de la filosofía.

Pero si esto fuera realmente posible (ya sabemos que no lo es) **¿Qué
implicaciones tendría para la lógica?**

Por un lado ¿Dónde quedaría la semántica? Si resulta que cualquier
proposición demostrable dentro de mi Teoría, fuera una proposición
verdadera, y que cualquier proposición verdadera fuera demostrable
dentro de la Teoría. Entonces ¿Para qué necesitamos la semántica? En mi
opinión se confundirían la verdad y la derivabilidad lógica.

Pero eso no implica que la semantica desapareciera del todo. Si
consideramos a la semántica como una correlación entre las ideas
(expresables en mi Teoría) y el mundo, entonces resulta que la semántica
dejaría de ser un criterio de conocimiento en favor a la
demostrabilidad, dado que, a efectos de conocimiento, ya no importaría
la verdad o falsedad de una proposición sino su derivabilidad lógica.
Pero quizá la semántica tendría aún alguna utilidad, por ejemplo a la
hora de ejecutar las ideas; pero si hemos axiomatizado el movimiento ¿no
será también este una cuestión sintáctica?... Entonces ¿Serviría de algo
la semántica? Para responder a esta cuestión supongo que habrá que tener
en cuenta la distinción entre partículas lógicas y partículas no
lógicas, las primeras son los operadores booleanos (y, o) y otros como
la implicación etc... (vean que voy un poco pensando en alto), las
últimas son constantes o variables, sólo respecto de este tipo de
partículas, las no lógicas, es necesaria la semántica. En mi opinión la
semántica tendría importancia en la formulización de los axiomas y en la
comprensión de las fórmulas derivadas. Todo se volvería exclusivamente
un problema de interpretación de las partículas no lógicas y quizá de
ahí venga la preocupación de algunos como Russell de definir nombres
unívocos etc...

Por otro lado **¿Que nos diría esto acerca de la estructura del mundo?**
Pues que el mundo estaría estructurado lógicamente, esto tiene que ser
así porque lo que estamos ambicionando es un sistema sintáctico con
capacidad comprensora, con capacidad de tener conocimiento acerca del
mundo, y no cualquier conocimiento sinó un conocimiento total; si el
mundo no se estructurase lógicamente este tipo de ambición sería
ilusoria. Pero pero esa estructura lógica sería la misma lógica que
usamos nosotros los humanos, hijos de una civiliación enana que está
perdida en la basta magnitud del Universo. Esto os puede parecer extraño
o no, alguno dirá, «bueno, a mi me resultaría sorprendente que el mundo
no tuviera ninguna lógica» ciertamente, pero lo que a mi me sorprendería
es que nosotros alcanzáramos **esa misma lógica** del Universo; la
cuestión es; ¿esa lógica que supuestamente tiene el mundo *es nuestra*?
¿No es un poco raro que nosotros siendo lo más minúsculo del Universo
tengamos esa ingente capacidad comprensora de TODO el Universo? ¿No será
más bien que *vemos las cosas como somos y no cómo realmente son*? Quizá
seamos nosotros los que ponemos ahí, delante de nosotros, nuestra lógica
con los materiales que el mundo nos ofrece. En mi opinión, el que
nosotros comprendamos la estructura lógica del mundo sería muy raro ya
que ¿Porqué vamos a venir nosotros equipados, ya de mano, con una lógica
del mundo? O quizás es que nosotros nos limitamos a aprenderla; pero
esto tampoco es menos sorprendente ¿Esa lógica del mundo será compatible
con la nuestra? Es decir, obviamente nosotros pensamos de forma lógica,
si algo no se adecúa a esa lógica no podemos entenderlo; nos sonará, y
no es casualidad que diga esto, a física cuántica o algo así ¿no se
producirán inconsistencias si intento introducir un esquema lógico
distinto a aquel con el que pienso? ¿Acaso podré modificar mi sistema
lógico desde la base? o ¿acaso tendré que construir una inteligencia
aritificial con esa lógica del mundo y a la que pueda preguntar? Aunque,
en mi opinión, esta lógica del mundo tendrá que ser algún tipo de
[lógica
paraconsistente](http://es.wikipedia.org/wiki/L%C3%B3gica_paraconsistente)
muy avanzada.

Bien, todo este proyecto se vio frustrado; a pesar de que algunos no
parecen querer verlo; por Gödel y sus Teoremas de incompletud. En mi
opinión lo que Gödel demostró es que la noción sintáctica de
demostrabilidad lógica y la noción semántica de verdad son distintas, no
se solapan en una Teoría axiomática con alta capacidad expresiva, **o no
todo lo demostrable es verdadero o no todo lo verdadero es
demostrable**. Por tanto, para tener un conocimiento completo tengo que
trabajar con ambas nociones y en última instancia lo que importa es lo
verdadero, porque sobre él voy a construir mi conocimiento, así nos
comportamos nosotros, o al menos eso parece. Esto tiene el problema de
que el mundo de lo verdadero es mucho más esquivo e inseguro que el
mundo de lo demostrable. Si en el mundo de lo demostrable mi
conocimiento sobre un tema viene apoyado en una demostración lógica,
algo que es unívoco, objetivo, firme. En el mundo de lo verdadero mi
conocimiento sobre un tema viene asentado en una *creencia verdadera y
justificada* ([a pesar de
Guettier](http://es.wikipedia.org/wiki/Problema_de_Gettier)); lo cual
nos deja en el terrible problema de cómo justifico una creencia. En
algunos casos será gracias a una demostración lógica, claro; pero como
hemos visto, esto no siempre será posible... Si tenemos en cuenta que,
como decía Wittgenstein, la mera presencia de algo no es fundamento de
nada, y que mi conocimiento pasado sobre un hecho [no es suficiente por
si sólo para justificar mi creencia sobre hechos
nuevos](http://www.filosoficas.unam.mx/%7Emariogt/2005cGomez-Torrente_WK&ARF.pdf);
nos quedamos atados a la única sólución posible que es convertir el
conocimiento en algo normativo, en algo que depende de mi comunidad.

Intentando extender las implicaciones del Teorema de Gödel hacia el
debate en torno a la posibilidad o no de una Inteligencia Artificial
semejante a la humana; algún autor, en concreto
[Penrose](http://es.wikipedia.org/wiki/Roger_Penrose), ha defendido que
precisamente el Teorema de Gödel excluye este proyecto dado que
demuestra que **hay verdades que son comprensibles intuitivamente por el
hombre y que, al no ser demostrables, nunca podrían ser asumidas por una
Inteligencia artificial**, que es, en principio, un aparato puramente
mecánico que sólo trabaja con una sintáctica. Este argumento falla en la
base porque realmente el Teorema de Gödel no demuestra la incompletud de
la aritmética, (esto es, que hay verdades que no son demostrables) sino
que «*si la aritmética es consistente* entonces es incompleta», pero es
que el último Teorema de Gödel dice que «Si la aritmética es consistente
entonces es indemostrable su consistencia» por lo tanto, el argumento de
Penrose no puede ser usado, dado que nos exige creer, como [petición de
principio,](http://es.wikipedia.org/wiki/Petici%C3%B3n_de_principio) que
la aritmética es consistente.

Ahora bien, esto me ha hecho pensar. Lo que yo me pregunto ahora es
¿**Una máquina de IA es un sistema puramente sintáctico o es capaz de
trabajar con una semántica**? Esta es en mi opinión una cuestión
importante para el debate sobre la IA.

**Si un sistema informático no puede trabajar con una semántica**
podrían suceder dos cosas, o que el mundo tuviera una estructura lógica
que fuera expresable en una Teoría axiomática, entonces estas máquinas
tendrían, o podrían tener una inteligencia similar a la humana; en este
caso habría que considerar la posibilidad de que **nosotros fueramos
mecanismos puramente sintácticos**, con una *minisemántica* dedicada a
la interpretación, pero que no fundamentaría el conocimiento. O puede
ser, por el contrario, que el mundo no tenga una estructura lógica y que
por tanto un aparato puramente sintáctico no pueda comprenderlo por
entero; en este caso el conocimiento se basaría en la semántica, y
entonces la semántica y la sintáctica se moverían en campos bien
distintos; además, como hemos postulado que la máquina no podrá entender
conceptos; entonces sencillamente una máquina nunca podría tener la
misma inteligencia que un ser humano.

**Pero es posible que las máquinas puedan empezar, en algún momento, a
manejar conceptos**; probablemente conceptos que ellas mismas hayan
creado, en una especie de [inteligencia
enjambre;](http://es.wikipedia.org/wiki/Inteligencia_de_enjambre)
considero que en este último caso, las máquinas podrían ser consideradas
inteligentes y empezarían a trabajar con una semántica creada
colectivamente. Esto no puede resultar extraño si tenemos en cuenta los
resultados de la investigación del segundo Wittgenstein; la semántica es
en esencia el resultado de juegos del lenguaje que se practícan en una
comunidad, por tanto la semántica es algo social. Es decir lo que
defiendo es que la semántica es un elemento emergente que surge de
distintos aparatos sintácticos que trabajan en una red distribuída y
crean o modifican por sí un lenguaje. Daros cuenta de que este último
requisito del enjambre es perfectamente factible a día de hoy internet
no es más que una [red
distribuída](http://es.wikipedia.org/wiki/Red_distribuida) de [máquinas
universales de
Turing](http://es.wikipedia.org/wiki/M%C3%A1quina_de_Turing); lo único
que les falta a estas máquinas es crear una semántica; y esto quizá
llegue con las [ontologías
web](http://es.wikipedia.org/wiki/Ontolog%C3%ADa_%28inform%C3%A1tica%29),
al menos estas serán la base de la misma.

Aprovecho para decir que, en mi opinión [la prueba de
Turing](http://es.wikipedia.org/wiki/Prueba_de_Turing), que ya
explicaremos, no demuestra nada y falla en muchos aspectos, por un lado
identifica la inteligencia con la inteligencia lingüística, cuando desde
[Andy Clark](http://en.wikipedia.org/wiki/Andy_Clark), esa
identificación es inexacta, por otro lado niega inteligencia a seres que
se han considerado con una inteligencia media, como los chimpancés, etc
y que obviamente no superarían la prueba. Por otro lado considera
inteligentes a sistemas que como
[Searle](http://en.wikipedia.org/wiki/John_Searle) [ha
demostrado](http://es.wikipedia.org/wiki/Habitaci%C3%B3n_china), están
lejos de serlo.

En mi opinión la prueba que deba demostrar capacidades inteligentes en
las máquinas ha de ser una prueba que se base en la compentencia o
incompetencia de las máquinas en cooperar en red con otras máquinas para
resolver problemas complejos para los que no estén programadas, usando
para ello conceptos propios; obviamente es una prueba de difícil diseño,
y que tendrá que tener en cuenta si están programadas para resolver
problemas similares y cual es exactamente el grado de similitud de esos
problemas.

La cuestión es ¿Podrán las máquinas crear una semántica propia? Porque
si es así, yo os puedo asegurar que tarde o temprano alcanzaran una
inteligencia no igual quizás, no exactamente igual a la nuestra, pero
probablemente comparable e incluso hasta superior.

Y no es por asustar pero quizás el proceso ya haya empezado... (aunque
claro, el usar nombres no es igual que entender conceptos.... Pero
[*habelas hainas*](http://es.wikipedia.org/wiki/Meiga))
